4. Testing & Quality Assurance (QA)
This section outlines the testing strategy, tooling, processes, and quality gates used throughout the development of SkinMatch. The objective is to ensure that the platform remains reliable, accurate, secure, and consistent with the intended user experience.

Test Objectives
- Verify backend APIs, business logic, and data handling against requirements.
- Validate frontend component behavior and user-facing flows.
- Detect regressions early via automated testing and CI.

Test Execution (commands)
- Backend: `pytest` (or `make test-pytest` from repo root)
- Frontend: `npm test -- --runInBand` (unit/integration), `npm run test:e2e` (Playwright)

Test Result Summary (current run)
- Backend: 128 passed (pytest, Python 3.12.12, Django 5.2.7).
- Frontend: 50 passed (Jest/RTL/Playwright).

4.1 Unit Testing
Scope
Unit tests validate small, deterministic parts of the system, including:
Pure functions (ingredient parsing, compatibility rules, quiz scoring)
React UI components with mocked hooks/services
Django services, models, and schema validators
Utility helpers such as:
Ingredient normalization
Profile scoring engine
Recommendation rule evaluator
Approach
Apply TDD where feasible, especially for algorithmic modules (ingredient scoring, profile analysis).
Keep tests deterministic through mocking of:
External I/O
File storage
HTTP requests
Random seeds
Keep unit tests isolated and side-effect–free.
Use snapshot tests only for stable UI atoms to reduce fragility.
Tooling
Backend: pytest, pytest-django, pytest-cov
Frontend: Jest + React Testing Library, MSW for API mocking
Linting: ESLint for frontend (backend linting planned)
Coverage: Enforced in CI via pytest --cov and jest --coverage
Acceptance Criteria
At least 90% coverage for:
Ingredient matrix
Recommendation engine
Routine planner
Zero flaky tests
Unit suite must complete in ≤ 90 seconds
All unit tests must pass before merging (GitHub Actions required check)
Artifacts
HTML/XML coverage reports
JUnit XML output for CI pipelines
Linting reports as CI quality gates
4.2 Integration Testing
Scope
Integration tests validate interactions across layers:
API endpoints (Django Ninja Extra)
PostgreSQL database queries
Caching for ingredient search and recommendations
React → API data-fetch flows
Authentication & authorization flows
File upload handling (product imagery, OCR ingredient extraction stub)
Approach
Backend integration via pytest with a live test database and migrations applied before tests.
Validate API routes using the Ninja test client:
Shape/schema validation
Pagination behavior
Token/permission handling
Frontend integration using RTL + MSW for simulated end-to-end calls.
E2E smoke flows with Playwright for browser-level coverage (e.g., quiz completion to recommendations).
Test Data
Controlled seed data: ingredients, products, user profiles, concerns
Use `python manage.py load_sample --reset` to seed ingredients/products/concerns for integration tests (run from backend).
Utilize load_sample fixtures when applicable
Temporary directories for uploaded files to keep test runs idempotent
Non-Functional Checks
Logging behaviors (analytics, auditing)
Cache invalidation correctness
Backward compatibility of API contracts:
Schema snapshots
OpenAPI diffing
Acceptance Criteria
All critical user journeys pass:
Onboarding and quiz flow
Skin profile creation
Product search, OCR analysis, ingredient guidance
Recommendation generation
Routine creation and editing
No API schema regressions
No orphaned data or unclean teardown
4.3 User Acceptance Testing (UAT)
Scope
End-to-end validation aligned with project requirements and user stories (see README roadmap/features), covering:
Onboarding and quiz experience
Ingredient guidance module
Personalized recommendations
Routine builder (AM/PM)
Product explorer, search, and similarity finder
Admin content updates
Analytics dashboard
Participants
Product owner / team lead
2–3 representative end users (e.g., oily, sensitive, acne-prone skin types)
Process
Conduct scripted test scenarios combined with exploratory testing.
Perform testing on staging with production-like (non-PII) data.
Capture:
Screenshots
Network logs
Console logs
Defects with severity labels
Re-run scenarios after fixes.
Entry Criteria
All features deployed to staging
All high-priority defects from previous cycles resolved
Exit Criteria
No open Sev1/Sev2 defects
All user flows fully validated and signed off
Minor cosmetic issues logged for later iterations
Checklists
Accessibility (keyboard navigation, ARIA labels, contrast ratio)
Mobile responsiveness (Android + iOS breakpoints)
Performance smoke tests (initial load under target)
Copy accuracy and translation checks
Validity of external product/affiliate links
4.4 Performance & Load Testing
Objectives
To ensure the system performs reliably under real-world and peak loads by measuring:
Backend latency & throughput
Frontend loading performance
Database query efficiency
Overall system resilience and stability
Performance Targets
Backend p95 latency:
Read: ≤ 300 ms
Write: ≤ 600 ms
Frontend:
First Contentful Paint (FCP) meets the budget under 3G Fast conditions
No unexpected bundle-size regressions
Error Rate: < 0.1% under load
Supports graceful degradation (retry/backoff)
Performance/Load Execution
- Not currently executed or automated. Use the targets above as reference when the team schedules a performance test cycle.
- Choose tooling (e.g., k6/Locust, Lighthouse) and run against a production-like environment when planned.
